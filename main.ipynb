{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 0 – CLEAN & DOWNLOAD EVERYTHING TO EXTERNAL HDD (run once)\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import os, shutil, zipfile\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# 0-a) set this to your external drive’s mount point + folder\n",
    "external_root = Path(\"/Volumes/Amogh's HDD/dataset\")  \n",
    "if not external_root.is_dir():\n",
    "    raise FileNotFoundError(f\"Drive path not found: {external_root}\")\n",
    "    \n",
    "# 0-b) cd into that folder\n",
    "os.chdir(external_root)\n",
    "print(\"Working directory:\", external_root)\n",
    "\n",
    "# 0-c) remove old dataset folders (won’t touch your .zip files)\n",
    "for name in (\"dataset1\",\"dataset2\",\"roboflow_pistols\",\"Armed Person with Rifle.v1i.yolov8\",\"classifier_armed_unarmed\"):\n",
    "    p = external_root/name\n",
    "    if p.exists():\n",
    "        print(\"🗑 Removing:\", p.name)\n",
    "        shutil.rmtree(p)\n",
    "\n",
    "# 0-d) authenticate Kaggle API\n",
    "api = KaggleApi(); api.authenticate()\n",
    "\n",
    "# 0-e) download & unzip issaisasank/guns-object-detection → ./dataset1\n",
    "print(\"⇣ Downloading guns-object-detection → dataset1/\")\n",
    "api.dataset_download_files(\"issaisasank/guns-object-detection\",\n",
    "                           path=\"dataset1\", unzip=True)\n",
    "print(\"✔ dataset1 ready\")\n",
    "\n",
    "# 0-f) download & unzip snehilsanyal/weapon-detection-test → ./dataset2\n",
    "print(\"⇣ Downloading weapon-detection-test → dataset2/\")\n",
    "api.dataset_download_files(\"snehilsanyal/weapon-detection-test\",\n",
    "                           path=\"dataset2\", unzip=True)\n",
    "print(\"✔ dataset2 ready\")\n",
    "\n",
    "# 0-g) extract any ZIPs in this folder into their own dirs\n",
    "for z in external_root.glob(\"*.zip\"):\n",
    "    out = external_root / z.stem\n",
    "    if out.exists(): shutil.rmtree(out)\n",
    "    print(f\"⇣ Extracting {z.name} → {out.name}/\")\n",
    "    with zipfile.ZipFile(z) as zf:\n",
    "        zf.extractall(out)\n",
    "    print(f\"✔ {out.name} ready\")\n",
    "\n",
    "print(\"\\n✅ All datasets are now under\", external_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 0-A – add Roboflow pistols ZIP to the external HDD dataset\n",
    "# ---------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import zipfile, shutil, os\n",
    "\n",
    "# Path to your external HDD dataset root (same one used in Step 0)\n",
    "base_dir = Path(\"/Volumes/Amogh's HDD/dataset\")   # edit if different\n",
    "if not base_dir.is_dir():\n",
    "    raise FileNotFoundError(f\"Dataset root not found: {base_dir}\")\n",
    "\n",
    "# 1️⃣ find a *.zip with “pistols” in its file name in the notebook folder\n",
    "\n",
    "zip_path = \"/Users/amogharya/Documents/gun detection multiple ds/roboflow_pistols.zip\"\n",
    "\n",
    "# 2️⃣ target extraction folder inside your HDD dataset\n",
    "out_folder = base_dir / \"roboflow_pistols\"\n",
    "if out_folder.exists():\n",
    "    shutil.rmtree(out_folder)\n",
    "print(f\"⇣ Extracting {zip_path} → {out_folder.name}/\")\n",
    "with zipfile.ZipFile(zip_path) as zf:\n",
    "    zf.extractall(out_folder)\n",
    "print(\"✔ roboflow_pistols ready at\", out_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 – MERGE + CLEAN (host machine, current directory)\n",
    "# --------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import shutil, random, yaml, os, sys\n",
    "\n",
    "root = Path.cwd()\n",
    "print(\"📂 Working directory:\", root)\n",
    "\n",
    "img_exts = {\".jpg\", \".jpeg\", \".png\"}\n",
    "pairs = []\n",
    "\n",
    "def label_for(img):\n",
    "    \"\"\"images/…/foo.jpg  ->  labels/…/foo.txt\"\"\"\n",
    "    parts = list(img.parts)\n",
    "    try:\n",
    "        idx = parts.index(\"images\")\n",
    "        parts[idx] = \"labels\"\n",
    "        return Path(*parts).with_suffix(\".txt\")\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# 1️⃣ Scan every image file under any *images* sub-tree\n",
    "dataset_hits = {}\n",
    "for img in root.rglob(\"*\"):\n",
    "    if img.suffix.lower() not in img_exts:         # not jpg/png\n",
    "        continue\n",
    "    if \"images\" not in img.parts:                  # not inside images/\n",
    "        continue\n",
    "\n",
    "    lbl = label_for(img)\n",
    "    if lbl and lbl.exists():\n",
    "        ds_name = img.parts[ img.parts.index(\"images\") - 1 ]  # folder above images\n",
    "        dataset_hits.setdefault(ds_name, 0)\n",
    "        dataset_hits[ds_name] += 1\n",
    "        pairs.append((ds_name, img, lbl))\n",
    "\n",
    "print(\"\\n🔍 Datasets discovered & pair counts:\")\n",
    "for k,v in dataset_hits.items():\n",
    "    print(f\"   {k:<25} {v} pairs\")\n",
    "print(f\"\\nTOTAL pairs found = {len(pairs)}\")\n",
    "\n",
    "# 2️⃣ Prepare YOLO skeleton (data/train, data/val)\n",
    "yolo_root = root / \"data\"\n",
    "if yolo_root.exists():\n",
    "    shutil.rmtree(yolo_root)\n",
    "for split in (\"train\", \"val\"):\n",
    "    (yolo_root/split/\"images\").mkdir(parents=True, exist_ok=True)\n",
    "    (yolo_root/split/\"labels\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 3️⃣ Shuffle & 80/20 split\n",
    "random.seed(42)\n",
    "random.shuffle(pairs)\n",
    "cut = int(0.8 * len(pairs))\n",
    "train_pairs, val_pairs = pairs[:cut], pairs[cut:]\n",
    "\n",
    "def copy_pairs(subset, split):\n",
    "    copied = 0\n",
    "    for tag, img, lbl in subset:\n",
    "        stem = f\"{tag}_{img.stem}\"\n",
    "        dst_img = yolo_root/split/\"images\"/f\"{stem}{img.suffix.lower()}\"\n",
    "        dst_lbl = yolo_root/split/\"labels\"/f\"{stem}.txt\"\n",
    "        shutil.copy2(img, dst_img)\n",
    "        shutil.copy2(lbl, dst_lbl)\n",
    "        copied += 1\n",
    "        if copied % 2000 == 0:\n",
    "            print(f\"   ⏳ copied {copied} → {split}\")  # progress log\n",
    "    print(f\"✔ {copied} files copied into {split}\")\n",
    "\n",
    "copy_pairs(train_pairs, \"train\")\n",
    "copy_pairs(val_pairs,   \"val\")\n",
    "\n",
    "# 4️⃣ Clean & unify labels (class → 0, drop bad rows / empty files)\n",
    "def clean(lbl_dir, img_dir):\n",
    "    drops = 0\n",
    "    for txt in lbl_dir.glob(\"*.txt\"):\n",
    "        lines = txt.read_text().splitlines()\n",
    "        good = []\n",
    "        for L in lines:\n",
    "            p = L.split()\n",
    "            if len(p) < 5:\n",
    "                continue\n",
    "            _, x, y, w, h = p[:5]\n",
    "            good.append(f\"0 {x} {y} {w} {h}\")\n",
    "        if good:\n",
    "            txt.write_text(\"\\n\".join(good) + \"\\n\")\n",
    "        else:\n",
    "            txt.unlink()\n",
    "            drops += 1\n",
    "            for ext in img_exts:\n",
    "                (img_dir/f\"{txt.stem}{ext}\").unlink(missing_ok=True)\n",
    "    return drops\n",
    "\n",
    "dropped_train = clean(yolo_root/\"train\"/\"labels\", yolo_root/\"train\"/\"images\")\n",
    "dropped_val   = clean(yolo_root/\"val\"/\"labels\",   yolo_root/\"val\"/\"images\")\n",
    "\n",
    "print(f\"🧹 Dropped {dropped_train} empty/bad labels from train, {dropped_val} from val\")\n",
    "\n",
    "# 5️⃣ Write data.yaml\n",
    "data_yaml = root/\"data.yaml\"\n",
    "yaml.safe_dump({\n",
    "    \"train\": str((yolo_root/\"train\"/\"images\").resolve()),\n",
    "    \"val\":   str((yolo_root/\"val\"/\"images\").resolve()),\n",
    "    \"nc\":    1,\n",
    "    \"names\": [\"gun\"]\n",
    "}, open(data_yaml, \"w\"))\n",
    "\n",
    "print(\"\\n📝 data.yaml saved →\", data_yaml)\n",
    "print(\"✅ Merge & clean complete — you can now run the training step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4 – Train YOLO-v8 NANO on the merged dataset (host Mac, no aug, imgsz = 352)\n",
    "# --------------------------------------------------------------------------------\n",
    "from ultralytics import YOLO\n",
    "import torch, time, warnings, multiprocessing as mp\n",
    "from pathlib import Path\n",
    "\n",
    "# 1️⃣  Paths\n",
    "root      = Path.cwd()                   # <— you copied the whole dataset here\n",
    "data_yaml = root / \"data.yaml\"           # written by Step 1\n",
    "assert data_yaml.exists(), f\"{data_yaml} not found\"\n",
    "\n",
    "runs_dir  = root / \"runs\"                # training outputs stay on your SSD\n",
    "runs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 2️⃣  Device\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"🖥  Training on:\", device)\n",
    "\n",
    "# 3️⃣  Model\n",
    "model = YOLO(\"yolov8n.pt\")               # nano → Pi-friendly\n",
    "\n",
    "# 4️⃣  Base config (no on-the-fly aug, disk cache)\n",
    "base = dict(\n",
    "    data       = str(data_yaml),\n",
    "    imgsz      = 352,          # divisible by 32, ~350\n",
    "    epochs     = 20,\n",
    "    device     = device,\n",
    "    cache      = \"disk\",\n",
    "    workers    = mp.cpu_count(),\n",
    "    amp        = True,\n",
    "    augment    = False,\n",
    "    close_mosaic = 0,\n",
    "    cos_lr     = True,\n",
    "    val        = False,        # skip per-epoch val to keep it fast\n",
    "    pretrained = True,\n",
    "    patience   = 5,\n",
    "    project    = str(runs_dir),\n",
    "    name       = \"gun_nano_352_host\",\n",
    "    verbose    = True,\n",
    "    plots      = False,\n",
    "    save_period = 1            # save weights every epoch\n",
    ")\n",
    "\n",
    "# 5️⃣  Try batches until one fits memory (16 → 8 → 4 → 2 → 1)\n",
    "def train_until_fits(batches=(16, 8, 4, 2, 1)):\n",
    "    for bs in batches:\n",
    "        try:\n",
    "            print(f\"\\n🚀  Launching training with batch = {bs}\")\n",
    "            t0 = time.time()\n",
    "            model.train(batch=bs, **base)\n",
    "            print(f\"✅  batch={bs} finished in {(time.time()-t0)/60:.1f} min\")\n",
    "            return\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                warnings.warn(f\"⚠️  OOM at batch {bs} – trying smaller …\")\n",
    "                if device == \"cuda\":\n",
    "                    torch.cuda.empty_cache()\n",
    "            else:\n",
    "                raise\n",
    "    raise RuntimeError(\"All batch sizes OOM — lower imgsz or free RAM\")\n",
    "\n",
    "train_until_fits()\n",
    "\n",
    "print(\"\\n🏁  Done!  Best weights →\",\n",
    "      runs_dir / \"gun_nano_352_host\" / \"weights\" / \"best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5 – quick test on arbitrary images\n",
    "# ---------------------------------------\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "\n",
    "# 1️⃣  folders\n",
    "src_dir = Path(\"test_imgs\")           # drop images here\n",
    "dst_dir = Path(\"test_out\")\n",
    "dst_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 2️⃣  load trained weights (edit path if your run name differs)\n",
    "weights = \"/Users/amogharya/Documents/gun detection multiple ds/best.pt\"\n",
    "model   = YOLO(weights)\n",
    "\n",
    "# 3️⃣  run inference on every image in test_imgs/\n",
    "print(\"\\nImage\".ljust(30), \"Result\")\n",
    "print(\"-\"*45)\n",
    "for img in sorted(src_dir.glob(\"*\")):\n",
    "    if img.suffix.lower() not in {\".jpg\", \".jpeg\", \".png\"}:\n",
    "        continue\n",
    "    r = model(img, imgsz=320, conf=0.35, save=True,\n",
    "              project=dst_dir, name=\"\")[0]\n",
    "    n = len(r.boxes)\n",
    "    status = f\"DETECTED ({n})\" if n else \"NOT-DETECTED\"\n",
    "    print(f\"{img.name.ljust(30)} {status}\")\n",
    "\n",
    "print(f\"\\n✔ Annotated copies saved in: {dst_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REAL-TIME WEAPON DETECTION WITH CONTINUOUS LOGGING & CLEAN SHUTDOWN\n",
    "# -----------------------------------------------------------------\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import logging                               # 🔄 NEW\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 📓 Configure continuous logging  🔄 NEW\n",
    "logging.basicConfig(\n",
    "    filename=\"weapon_detection.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "# 1️⃣ Select device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"🔌 Using device: {device}\")\n",
    "\n",
    "# 2️⃣ Load your best weights\n",
    "model = YOLO(\"/Users/amogharya/Documents/gun detection multiple ds/best.pt\")  # adjust path if needed\n",
    "\n",
    "# 🔄 NEW -------------------------------------------------------------\n",
    "def open_camera() -> cv2.VideoCapture | None:\n",
    "    \"\"\"\n",
    "    Try several back-ends / indices so the script works on macOS (AVFoundation),\n",
    "    Windows (MSMF) and Linux (V4L2).\n",
    "    \"\"\"\n",
    "    tried = []\n",
    "    # List of (index, backend) pairs to test\n",
    "    candidates = [\n",
    "        (0, cv2.CAP_AVFOUNDATION),   # macOS\n",
    "        (0, cv2.CAP_MSMF),           # Windows 10+\n",
    "        (0, cv2.CAP_DSHOW),          # Windows fallback\n",
    "        (0, cv2.CAP_V4L2),           # Linux\n",
    "        (0, None),                   # OpenCV default\n",
    "        (1, None), (2, None)         # extra indices if multiple cameras\n",
    "    ]\n",
    "    for idx, backend in candidates:\n",
    "        if backend is None:\n",
    "            cap = cv2.VideoCapture(idx)\n",
    "            tried.append(f\"{idx}/DEFAULT\")\n",
    "        else:\n",
    "            cap = cv2.VideoCapture(idx, backend)\n",
    "            tried.append(f\"{idx}/{backend}\")\n",
    "        if cap.isOpened():\n",
    "            print(f\"📸  Camera opened with {tried[-1]}\")\n",
    "            return cap\n",
    "        cap.release()\n",
    "    print(f\"❌ Tried camera back-ends: {', '.join(tried)}\")\n",
    "    return None\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# 3️⃣ Open webcam\n",
    "cap = open_camera()                           # 🔄 NEW\n",
    "if cap is None:\n",
    "    raise RuntimeError(\"⚠️ Could not open webcam\")\n",
    "\n",
    "print(\"▶️  Starting real-time detection. Press q or Ctrl+C to stop.\")\n",
    "\n",
    "prev_time = time.time()\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"⚠️  Frame read failed\")\n",
    "            logging.warning(\"Frame read failed\")   # 🔄 NEW\n",
    "            break\n",
    "\n",
    "        # 4️⃣ Inference\n",
    "        results = model(frame, device=device, imgsz=320, conf=0.30)[0]\n",
    "        n = len(results.boxes)\n",
    "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"[{timestamp}] Detected {n} object(s)\")\n",
    "        logging.info(f\"Detected {n} object(s)\")     # 🔄 NEW\n",
    "\n",
    "        # 5️⃣ Draw boxes + confidences\n",
    "        for box in results.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            conf = float(box.conf[0])\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{conf:.2f}\", (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            logging.info(                          # 🔄 NEW\n",
    "                f\"Box {x1},{y1},{x2},{y2} conf={conf:.2f}\"\n",
    "            )\n",
    "\n",
    "        # 6️⃣ Compute & display FPS\n",
    "        now = time.time()\n",
    "        fps = 1 / (now - prev_time)\n",
    "        prev_time = now\n",
    "        cv2.putText(frame, f\"FPS: {fps:.1f}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)\n",
    "\n",
    "        # 7️⃣ Show the frame\n",
    "        cv2.imshow(\"Real-Time Gun Detection\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"❎  Quit requested, exiting.\")\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n✋ Interrupted by user. Shutting down.\")\n",
    "\n",
    "finally:\n",
    "    # 8️⃣ Cleanup\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"✅ Camera released and all windows closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751930820.892451 24835907 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4 Pro\n",
      "W0000 00:00:1751930820.947897 24959512 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1751930820.954273 24959523 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m img_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# --- YOLO inference\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[43myolo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_rgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONF_THRES\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m box \u001b[38;5;129;01min\u001b[39;00m detections\u001b[38;5;241m.\u001b[39mboxes:\n\u001b[1;32m     39\u001b[0m     cls_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(box\u001b[38;5;241m.\u001b[39mcls\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/Documents/gun detection multiple ds/.venv/lib/python3.12/site-packages/ultralytics/engine/model.py:555\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 555\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/gun detection multiple ds/.venv/lib/python3.12/site-packages/ultralytics/engine/predictor.py:227\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/gun detection multiple ds/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/gun detection multiple ds/.venv/lib/python3.12/site-packages/ultralytics/engine/predictor.py:330\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 330\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/gun detection multiple ds/.venv/lib/python3.12/site-packages/ultralytics/engine/predictor.py:182\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    177\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    178\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    181\u001b[0m )\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/gun detection multiple ds/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/gun detection multiple ds/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/gun detection multiple ds/.venv/lib/python3.12/site-packages/ultralytics/nn/autobackend.py:636\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed, **kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[0;32m--> 636\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[0;32m~/Documents/gun detection multiple ds/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/gun detection multiple ds/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/gun detection multiple ds/.venv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:138\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/gun detection multiple ds/.venv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:156\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/gun detection multiple ds/.venv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:179\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 179\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    180\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/Documents/gun detection multiple ds/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/gun detection multiple ds/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/gun detection multiple ds/.venv/lib/python3.12/site-packages/ultralytics/nn/modules/conv.py:92\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     83\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    Apply convolution and activation without batch normalization.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/gun detection multiple ds/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/gun detection multiple ds/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/gun detection multiple ds/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/gun detection multiple ds/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ======================= Block #1: dependency + demo =======================\n",
    "# ❶  Install requirements  (comment these out if already installed)\n",
    "# %pip install --upgrade ultralytics==8.2.0 mediapipe opencv-python --quiet\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# -------- YOLOv8 initialisation ------------------------------------------------\n",
    "yolo_model = YOLO(Path(\"best.pt\"))   # make sure best.pt is in the working dir\n",
    "CONF_THRES = 0.40                    # we'll tweak later\n",
    "\n",
    "# -------- MediaPipe Pose initialisation ---------------------------------------\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=1,\n",
    "    enable_segmentation=False,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    ")\n",
    "\n",
    "# -------- Video loop -----------------------------------------------------------\n",
    "cap = cv2.VideoCapture(0)            # change to \"sample.mp4\" if needed\n",
    "assert cap.isOpened(), \"Cannot open video source\"\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # --- YOLO inference\n",
    "    detections = yolo_model.predict(img_rgb, verbose=False, conf=CONF_THRES)[0]\n",
    "    for box in detections.boxes:\n",
    "        cls_id = int(box.cls.item())\n",
    "        conf   = float(box.conf.item())\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        label = f\"{yolo_model.names[cls_id]} {conf:.2f}\"\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "        cv2.putText(frame, label, (x1, y1-6), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "\n",
    "    # --- MediaPipe Pose inference\n",
    "    res = pose.process(img_rgb)\n",
    "    if res.pose_landmarks:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(\n",
    "            frame, res.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    cv2.imshow(\"YOLO + MediaPipe demo\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:   # ESC to quit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "# ==========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1752008318.900217 25541904 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4 Pro\n",
      "W0000 00:00:1752008318.950182 25549411 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752008318.955101 25549412 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "16:58:48 INFO: 10-s summary ➜  RED(gun+pose)=0  ORANGE(gun)=32  GREEN(pose)=0  None=106\n",
      "16:58:58 INFO: 10-s summary ➜  RED(gun+pose)=5  ORANGE(gun)=49  GREEN(pose)=3  None=62\n",
      "16:59:08 INFO: 10-s summary ➜  RED(gun+pose)=0  ORANGE(gun)=75  GREEN(pose)=0  None=61\n",
      "16:59:19 INFO: 10-s summary ➜  RED(gun+pose)=3  ORANGE(gun)=14  GREEN(pose)=33  None=79\n",
      "16:59:29 INFO: 10-s summary ➜  RED(gun+pose)=0  ORANGE(gun)=0  GREEN(pose)=0  None=136\n",
      "16:59:39 INFO: 10-s summary ➜  RED(gun+pose)=0  ORANGE(gun)=0  GREEN(pose)=0  None=130\n",
      "16:59:49 INFO: 10-s summary ➜  RED(gun+pose)=0  ORANGE(gun)=7  GREEN(pose)=15  None=104\n",
      "16:59:59 INFO: 10-s summary ➜  RED(gun+pose)=0  ORANGE(gun)=9  GREEN(pose)=0  None=118\n",
      "17:00:09 INFO: 10-s summary ➜  RED(gun+pose)=0  ORANGE(gun)=11  GREEN(pose)=0  None=112\n",
      "17:00:19 INFO: 10-s summary ➜  RED(gun+pose)=0  ORANGE(gun)=37  GREEN(pose)=11  None=73\n",
      "17:00:29 INFO: 10-s summary ➜  RED(gun+pose)=0  ORANGE(gun)=52  GREEN(pose)=0  None=72\n",
      "17:00:39 INFO: 10-s summary ➜  RED(gun+pose)=0  ORANGE(gun)=89  GREEN(pose)=0  None=28\n",
      "17:00:49 INFO: 10-s summary ➜  RED(gun+pose)=0  ORANGE(gun)=62  GREEN(pose)=0  None=67\n",
      "17:00:59 INFO: 10-s summary ➜  RED(gun+pose)=0  ORANGE(gun)=77  GREEN(pose)=0  None=34\n",
      "17:01:09 INFO: 10-s summary ➜  RED(gun+pose)=0  ORANGE(gun)=68  GREEN(pose)=0  None=45\n",
      "17:01:19 INFO: 10-s summary ➜  RED(gun+pose)=0  ORANGE(gun)=43  GREEN(pose)=0  None=84\n",
      "17:01:29 INFO: 10-s summary ➜  RED(gun+pose)=0  ORANGE(gun)=66  GREEN(pose)=0  None=59\n",
      "17:01:39 INFO: 10-s summary ➜  RED(gun+pose)=15  ORANGE(gun)=103  GREEN(pose)=6  None=7\n",
      "17:01:49 INFO: 10-s summary ➜  RED(gun+pose)=0  ORANGE(gun)=83  GREEN(pose)=0  None=47\n",
      "17:01:59 INFO: 10-s summary ➜  RED(gun+pose)=0  ORANGE(gun)=28  GREEN(pose)=28  None=59\n",
      "17:02:09 INFO: 10-s summary ➜  RED(gun+pose)=3  ORANGE(gun)=9  GREEN(pose)=62  None=49\n",
      "17:02:19 INFO: 10-s summary ➜  RED(gun+pose)=0  ORANGE(gun)=3  GREEN(pose)=0  None=118\n",
      "17:02:29 INFO: 10-s summary ➜  RED(gun+pose)=0  ORANGE(gun)=0  GREEN(pose)=0  None=142\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 136\u001b[0m\n\u001b[1;32m    132\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOSE-SHOOT\u001b[39m\u001b[38;5;124m\"\u001b[39m, (bx[\u001b[38;5;241m0\u001b[39m], bx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m),\n\u001b[1;32m    133\u001b[0m                 cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.5\u001b[39m, colour, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    135\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGun-Pose cascade\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame)\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m&\u001b[39m\u001b[38;5;241m0xFF\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m27\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m   \u001b[38;5;66;03m# ESC\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# ---- 10-s logging -------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t_window_start \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ======================= Block #3: colours + 10-s logs ======================\n",
    "import cv2, math, time, logging, numpy as np\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- CONFIG -----------------------------------------------------------------\n",
    "YOLO_CONF   = 0.40         \n",
    "ELBOW_MINDEG= 140\n",
    "EXT_RATIO   = 1.10\n",
    "POSE_DET_TH = 0.50\n",
    "DEVICE_VID  = 0             # webcam; change to \"clip.mp4\" etc.\n",
    "\n",
    "# ---------- LOGGING ----------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level    = logging.INFO,\n",
    "    format   = \"%(asctime)s %(levelname)s: %(message)s\",\n",
    "    datefmt  = \"%H:%M:%S\",\n",
    ")\n",
    "\n",
    "# ---------- Models -----------------------------------------------------------------\n",
    "yolo = YOLO(Path(\"best.pt\"))\n",
    "mp_pose = mp.solutions.pose\n",
    "pose    = mp_pose.Pose(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=1,\n",
    "    enable_segmentation=False,\n",
    "    min_detection_confidence=POSE_DET_TH,\n",
    "    min_tracking_confidence=0.5,\n",
    ")\n",
    "\n",
    "# ---------- Helper functions -------------------------------------------------------\n",
    "def _angle(a,b,c):\n",
    "    ab = np.array([a.x-b.x, a.y-b.y])\n",
    "    cb = np.array([c.x-b.x, c.y-b.y])\n",
    "    return math.degrees(\n",
    "        math.acos(\n",
    "            np.clip(np.dot(ab,cb) /\n",
    "                    ((np.linalg.norm(ab)*np.linalg.norm(cb))+1e-7), -1.0, 1.0)\n",
    "        )\n",
    "    )\n",
    "\n",
    "def arm_aiming(lm, side):\n",
    "    if side==\"L\":\n",
    "        shoulder, elbow, wrist, hip = lm[11], lm[13], lm[15], lm[23]\n",
    "    else:\n",
    "        shoulder, elbow, wrist, hip = lm[12], lm[14], lm[16], lm[24]\n",
    "    if min(shoulder.visibility, elbow.visibility,\n",
    "           wrist.visibility, hip.visibility) < 0.5:\n",
    "        return False\n",
    "    elbow_deg = _angle(shoulder, elbow, wrist)\n",
    "    torso_len = math.hypot(shoulder.x-hip.x, shoulder.y-hip.y)\n",
    "    arm_len   = math.hypot(shoulder.x-wrist.x, shoulder.y-wrist.y)\n",
    "    return elbow_deg >= ELBOW_MINDEG and arm_len/(torso_len+1e-7) >= EXT_RATIO\n",
    "\n",
    "def hand_box(lm, side, shape, pad=40):\n",
    "    wrist = lm[15] if side==\"L\" else lm[16]\n",
    "    h,w = shape[:2]\n",
    "    cx,cy = int(wrist.x*w), int(wrist.y*h)\n",
    "    return [max(cx-pad,0), max(cy-pad,0), min(cx+pad,w-1), min(cy+pad,h-1)]\n",
    "\n",
    "# ---------- Stats & timers ---------------------------------------------------------\n",
    "STATS = dict(GUN_POSE=0, GUN_ONLY=0, POSE_ONLY=0, NONE=0)\n",
    "t_window_start = time.time()\n",
    "\n",
    "def log_and_reset():\n",
    "    logging.info(\n",
    "        \"10-s summary ➜  RED(gun+pose)=%d  ORANGE(gun)=%d  GREEN(pose)=%d  None=%d\",\n",
    "        STATS[\"GUN_POSE\"], STATS[\"GUN_ONLY\"],\n",
    "        STATS[\"POSE_ONLY\"], STATS[\"NONE\"])\n",
    "    for k in STATS: STATS[k]=0\n",
    "\n",
    "# ---------- Colours (BGR) ----------------------------------------------------------\n",
    "CLR_RED    = (  0,  0,255)\n",
    "CLR_ORANGE = (  0,165,255)\n",
    "CLR_GREEN  = (  0,255,  0)\n",
    "\n",
    "# ---------- Video loop -------------------------------------------------------------\n",
    "cap = cv2.VideoCapture(DEVICE_VID)\n",
    "assert cap.isOpened(), \"Cannot open video source\"\n",
    "\n",
    "while True:\n",
    "    ok, frame = cap.read()\n",
    "    if not ok: break\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # ---- YOLO pass ----------------------------------------------------------------\n",
    "    gun_boxes=[]\n",
    "    res_det = yolo.predict(img_rgb, verbose=False, conf=YOLO_CONF)[0]\n",
    "    for b in res_det.boxes:\n",
    "        x1,y1,x2,y2 = map(int, b.xyxy[0])\n",
    "        gun_boxes.append((x1,y1,x2,y2,float(b.conf.item())))\n",
    "\n",
    "    # ---- Pose pass ----------------------------------------------------------------\n",
    "    res_pose = pose.process(img_rgb)\n",
    "    aiming   = False\n",
    "    aiming_side = None\n",
    "    if res_pose.pose_landmarks:\n",
    "        lm = res_pose.pose_landmarks.landmark\n",
    "        left = arm_aiming(lm,\"L\")\n",
    "        right= arm_aiming(lm,\"R\")\n",
    "        aiming = left or right\n",
    "        aiming_side = \"L\" if left else (\"R\" if right else None)\n",
    "        mp.solutions.drawing_utils.draw_landmarks(\n",
    "            frame, res_pose.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # ---- Decision & drawing -------------------------------------------------------\n",
    "    if gun_boxes and aiming:\n",
    "        event = \"GUN_POSE\"\n",
    "        colour = CLR_RED\n",
    "    elif gun_boxes:\n",
    "        event = \"GUN_ONLY\"\n",
    "        colour = CLR_ORANGE\n",
    "    elif aiming:\n",
    "        event = \"POSE_ONLY\"\n",
    "        colour = CLR_GREEN\n",
    "    else:\n",
    "        event = \"NONE\"\n",
    "\n",
    "    STATS[event] += 1    # accumulate stats\n",
    "\n",
    "    # Draw gun boxes\n",
    "    for (x1,y1,x2,y2,conf) in gun_boxes:\n",
    "        cv2.rectangle(frame, (x1,y1), (x2,y2), colour, 2)\n",
    "        cv2.putText(frame, f\"gun {conf:.2f}\", (x1,y1-6),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.55, colour, 2)\n",
    "\n",
    "    # Draw hand box if pose aiming\n",
    "    if aiming_side:\n",
    "        bx = hand_box(lm, aiming_side, frame.shape)\n",
    "        cv2.rectangle(frame, (bx[0],bx[1]), (bx[2],bx[3]), colour, 2)\n",
    "        cv2.putText(frame, \"POSE-SHOOT\", (bx[0], bx[1]-6),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour, 2)\n",
    "\n",
    "    cv2.imshow(\"Gun-Pose cascade\", frame)\n",
    "    if cv2.waitKey(1)&0xFF==27: break   # ESC\n",
    "\n",
    "    # ---- 10-s logging -------------------------------------------------------------\n",
    "    if time.time() - t_window_start >= 10:\n",
    "        log_and_reset()\n",
    "        t_window_start = time.time()\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.156 🚀 Python-3.12.9 torch-2.7.1 CPU (Apple M4 Pro)\n",
      "Model summary (fused): 92 layers, 25,840,339 parameters, 0 gradients, 78.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 194.7±61.2 MB/s, size: 77.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/amogharya/Documents/gun detection multiple ds/data/val/labels.cache... 2046 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2046/2046 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0m/Users/amogharya/Documents/gun detection multiple ds/data/val/images/train_0137710960addb4e_jpg.rf.48478cdbb6802c6463a44a14ebff6b15.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 128/128 [05:13<00:00,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2046       4861      0.878      0.782      0.878      0.606\n",
      "Speed: 0.1ms preprocess, 150.9ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "\n",
      "──────── Validation summary ────────\n",
      "Mean Precision (mP)       : 0.878\n",
      "Mean Recall    (mR)       : 0.782\n",
      "mAP@0.50                   : 0.878\n",
      "mAP@0.50-0.95             : 0.606\n",
      "📊  Saved per-class bar chart ➜ class_pr_bar.png\n",
      "WARNING ⚠️ ConfusionMatrix plot failure: ConfusionMatrix.plot() got an unexpected keyword argument 'show_text'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'set_size_inches'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m cm \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mconfusion_matrix\n\u001b[1;32m     61\u001b[0m fig \u001b[38;5;241m=\u001b[39m cm\u001b[38;5;241m.\u001b[39mplot(normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, show_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, colorbar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 62\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_size_inches\u001b[49m(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     63\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNormalized Confusion Matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     64\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'set_size_inches'"
     ]
    }
   ],
   "source": [
    "# STEP 5 – Post-training evaluation and pretty plots (works on Ultralytics ≥ 8.3)\n",
    "# ------------------------------------------------------------------------------\n",
    "#\n",
    "# • Runs `model.val()` on your val-split.\n",
    "# • Prints mean Precision/Recall/mAP.\n",
    "# • Saves two figures:\n",
    "#     1)  bar chart  (per-class Precision & Recall + mAP@0.50)\n",
    "#     2)  confusion-matrix heat-map  (if the matrix is exposed)\n",
    "#\n",
    "# Usage:\n",
    "#   – Place this cell in the same notebook after STEP 4 finishes.\n",
    "#   – Adjust `weights_path` / `run_name` if you changed them.\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ── 1  paths ──────────────────────────────────────────────────────────────\n",
    "root         = Path.cwd()\n",
    "weights_path = root /\"best.pt\"\n",
    "data_yaml    = root / \"data.yaml\"\n",
    "assert weights_path.exists(),  f\"{weights_path} not found\"\n",
    "assert data_yaml.exists(),     f\"{data_yaml} not found\"\n",
    "\n",
    "# ── 2  validate ──────────────────────────────────────────────────────────\n",
    "model   = YOLO(weights_path)\n",
    "metrics = model.val(data=str(data_yaml), plots=False, verbose=False)\n",
    "\n",
    "box = metrics.box       # shorthand\n",
    "\n",
    "mp, mr, map50, map_all = box.mp, box.mr, box.map50, box.map\n",
    "print(\"\\n──────── Validation summary ────────\")\n",
    "print(f\"Mean Precision (mP)       : {mp:.3f}\")\n",
    "print(f\"Mean Recall    (mR)       : {mr:.3f}\")\n",
    "print(f\"mAP@0.50                   : {map50:.3f}\")\n",
    "print(f\"mAP@0.50-0.95             : {map_all:.3f}\")\n",
    "\n",
    "# ── 3  per-class bar chart  (works even for 1-class models) ──────────────\n",
    "cls_names = getattr(model, \"names\", {i:str(i) for i in range(len(box.p))})\n",
    "indices   = np.arange(len(box.p))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(indices-0.2, box.p, width=0.4, label=\"Precision\")\n",
    "plt.bar(indices+0.2, box.r, width=0.4, label=\"Recall\")\n",
    "plt.xticks(indices, [cls_names[i] for i in indices])\n",
    "plt.ylim(0,1); plt.grid(axis=\"y\", ls=\"--\", alpha=.3)\n",
    "plt.ylabel(\"Score\"); plt.title(\"Per-class Precision / Recall\")\n",
    "for i,(p,r) in enumerate(zip(box.p, box.r)):\n",
    "    plt.text(i-0.22, p+0.02, f\"{p:.2f}\", fontsize=8)\n",
    "    plt.text(i+0.02, r+0.02, f\"{r:.2f}\", fontsize=8)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"class_pr_bar.png\", dpi=150)\n",
    "plt.close()\n",
    "print(\"📊  Saved per-class bar chart ➜ class_pr_bar.png\")\n",
    "\n",
    "# ── 4  confusion-matrix heat-map (if available) ─────────────────────────\n",
    "if hasattr(metrics, \"confusion_matrix\"):\n",
    "    cm = metrics.confusion_matrix\n",
    "    fig = cm.plot(normalize=True, show_text=True, colorbar=True)\n",
    "    fig.set_size_inches(4,4)\n",
    "    plt.title(\"Normalized Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"conf_matrix.png\", dpi=150)\n",
    "    plt.close()\n",
    "    print(\"📊  Saved confusion matrix ➜ conf_matrix.png\")\n",
    "else:\n",
    "    print(\"ℹ️  confusion_matrix attribute not present – skipping CM plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
